% IDF cosine
我們想要幫所有的$s_{i,j}$做clustering和ranking，
建出合理的學習流程。

令 $W$ 代表整個相關文件 $D'$ 的單字集，包含所有 unigram 及 bigram。
則 \textproc{GenerateFlow} 演算法如下：

\begin{algorithmic}
  \Function{GenerateFlow}{$D'$}
    \For{$w\in W$}
      \State $\mathit{bucket}(w)$ $\gets$ $\{d\,|\,d\in D',\,w\in d'\}$
    \EndFor
    \State $G$ $\gets$ Graph with $V:=D'$ and $E=\emptyset$
    \For{$w\in (W\setminus \text{\textproc{Buzzwords}})$}
      \For{all\;
           \begin{minipage}[t]{0.19\textwidth}
             $d_i,d_j\in \mathit{bucket}(w)$, \\
             $d(d_i,d_j) < \text{\textproc{Threshold}}$
           \end{minipage}}
        \State Connect $d_i$ and $d_j$ in $G$
      \EndFor
    \EndFor
    \State \textit{groups} $\gets$ connected components of $G$
    \State
    \State Sort \textit{groups} by $\dfrac{\sum \mathit{Rank}(s_{i,j})}{|\mathit{group}|\times |\{d_i \,|\, \exists s_{i,j} \in group\}|}$
    \State
    \State Remove groups of size $|\mathit{group}|\le\text{\textproc{Group Size}}$
  \EndFunction
\end{algorithmic}


\subsubsection{Clustering with word vector}

我們定義兩個$s_{i,j}$的距離如下。
如同 VSM，每個 $s_{i,j}$ 拆成很多word（實驗中包含 unigram 及 bigram），
並建立 word-indexed vector $v' = (\mathit{weight}(w))_w$。
對於每一個 word $w$， $\mathit{weight}(w):=\mathsf{TF}_w\times\mathsf{IDF}_w$
（其中 $\mathsf{TF}_w$ 僅計算 $w$ 出現在 $s_{i,j}$ 中的次數）。
最後我們用兩個 normalized-vector $v_{ij}:=v_{ij}'/||v_{ij}'||$, $v_{pq}:=v_{pq}'/||v_{pq}'||$ 之間的 cosine distance 來當$s_{i,j}$、$s_{p,q}$之間的相似度。

接著以雙層迴圈$O(N^{2})$檢查所有的$s_{i,j}$。
對於所有 $i,j,p,q$，若 $\cos(v_{ij},v_{pq}) > \text{\textproc{Threshold}}$就將兩個$s_{i,j}$, $s_{p,q}$連邊。

最後我們取大小超過\textproc{Group Size}的connected components
作為我們的cluster完的目錄條目。

\subsubsection{Bucket Optimization}
% bucket 加速 grouping
因為對於不少關鍵字如 \texttt{machine learning}、\texttt{algorithm} 等，
相關文件 $D'$ 的目錄 $\{s_{i,j}\}$數量個數 $N$ 約為 $10^4$ 量級，
因此$O(N^{2})$的算法效率有所不足（尤其本系統以python實作）。
作為優化，我們用字集 $W$ 建立 $\mathit{bucket}$，
並在 $bucket(w)$ 中存入所有包含 $w$ 的項目 $s_{i,j}$。
接著我們便可將 2.3.1 中建邊的動作單獨限制於每個bucket中。

假設每個bucket中$s_{i,j}$個數為平均為$m$個，且 $n\approx |s_{i,j}|$，
由於每個$s_{i,j}$約出現在$2|s_{i,j}|$個bucket中，
所以計算的複雜度會變成$O(m^2\times \frac{N\times n}{m}) = O(mnN)$。
實驗結果平均$|s_{i,j}| = 5,m = 10$，
因此絕大多數時間演算法效率接近 $O(N)$。

\noindent \paragraph{正確性}
如果$\cos(v_{ij},v_{pq})>\text{\textproc{Threshold}}$，
表示 $v_{ij}$ 與 $v_{pq}$ 至少有一個相同單字（不然$\cos(v_{ij},v_{pq})=0$）。
令 $w$ 為 $s_{i,j}$ 與 $s_{p,q}$ 的共同單字，則我們有 $s_{i,j},s_{p,q}\in \mathit{bucket}(w)$。因此建邊的演算法並不會少建（或多建）、連通塊仍然一樣。

\subsubsection{Group Ranking}
% ranking 排法
對於每一個$s_{i,j}$，
我們可以利用其原本在書中目錄的排序 $j/|d_{i,j}|$來當作其在$d_{i,j}$中的rank。
每個group便可以其擁有的所有$s_{i,j}$的rank做平均來排序。

但實驗中亦發現存在 $w\in s_{i,j}$ 滿足 $w$ 只於某本書大量出現，
而且章節也很前面，導致 $s_{i,j}$ 的 group 雖然不是重要內容，卻被排序到前面。
所以我們將group中有幾個「不同」的$d_{i,j}$ 也考慮進去，並以此penalize僅出現於少數$d_{i,j}$的group。

最後group的rank我們定義如下。
\[ \frac
    {\sum \mathit{Rank}(s_{i,j})}
    {|\mathit{group}|\times |\{d_i \,|\, \exists s_{i,j} \in group\}|} \]

\subsubsection{Buzzword 過濾}
% buzzwords 的過濾
Clustering過程中也發現
很多$s_{i,j}$被一些教科書、課程常出現的 \textproc{Buzzwords} 
像是Introduction、Chapter、Midterm等等cluster在一起。
我們可以透過直接忽略 \textproc{Buzzwords} 的bucket 解決這個問題，
避免 $s_{i,j}$利用buzzword被cluster在一起。而未被 cluster 在一起的
$s_{i,j}$ 會被 \textproc{Group Size} 的門檻移除。