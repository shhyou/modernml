% IDF cosine
我們想要幫所有的$s_{i,j}$做clustering和ranking，
建出合理的學習流程。

令 $W$ 代表整個相關文件 $D'$ 的單字集，包含所有 unigram 及 bigram。
則 \textproc{GenerateFlow} 演算法如下：

\begin{algorithmic}
  \Function{GenerateFlow}{$D'$}
    \For{$w\in W$}
      \State $\mathit{bucket}(w)$ $\gets$ $\{d\,|\,d\in D',\,w\in d'\}$
    \EndFor
    \State $G$ $\gets$ Graph with $V:=D'$ and $E=\emptyset$
    \For{$w\in (W\setminus \text{\textproc{Buzzwords}})$}
      \For{all\;
           \begin{minipage}[t]{0.19\textwidth}
             $d_i,d_j\in \mathit{bucket}(w)$, \\
             $d(d_i,d_j) < \text{\textproc{Threshold}}$
           \end{minipage}}
        \State Connect $d_i$ and $d_j$ in $G$
      \EndFor
    \EndFor
    \State \textit{groups} $\gets$ connected components of $G$
    \State
    \State Sort \textit{groups} by $\dfrac{\sum \mathit{Rank}(s_{i,j})}{|\mathit{group}|\times |\{d_i \,|\, \exists s_{i,j} \in group\}|}$
    \State
    \State Remove groups of size $|\mathit{group}|\le\text{\textproc{Group Size}}$
  \EndFunction
\end{algorithmic}


\subsubsection{Clustering with word vector}

我們定義兩個$s_{i,j}$的距離如下。
如同 VSM，每個 $s_{i,j}$ 拆成很多word（實驗中包含 unigram 及 bigram），
並建立 word-indexed vector $v' = (\mathit{weight}(w))_w$。
對於每一個 word $w$， $\mathit{weight}(w):=\mathsf{TF}_w\times\mathsf{IDF}_w$
（其中 $\mathsf{TF}_w$ 僅計算 $w$ 出現在 $s_{i,j}$ 中的次數）。
最後我們用兩個 normalized-vector $v_i'/||v_i'||$, $v_j'/||v_j'||$ 之間的 cosine distance 來當兩個$s_{i,j}$之間的相似度。

接著就可以$O(N^{2})$掃過所有個$s_{i,j}$，
對於兩個$s_{i,j}$的vector $v_1,v_2$，
如果$\cos(v_1,v_2) > \text{\textproc{Threshold}}$就將兩個$s_{i,j}$連邊，
最後我們取大小大於\textproc{Group Size}的connected components，
來當作我們的cluster完的groups。

\subsubsection{利用bucket加速}
% bucket 加速 grouping
因為對於不少關鍵字，
搜尋出來的$s_{i,j}$數量級約莫在$10^4$左右，
因此$O(N^{2})$的算法速度會稍嫌慢(尤其我們使用python)，
我們利用有出現的word先做$\mathit{bucket}$，
如果$s_{i,j}$有出現word $w$，那就會被丟到$bucket_w$中，
建邊的動作只在每個bucket中做，
如果平均每個bucket有$m$個$s_{i,j}$，
每個$s_{i,j}$出現在$|s_{i,j}|$個bucket中，
這樣我們計算的複雜度會變成$O(m^2\times \frac{N\times n}{m}) = O(mnN)$，
統計上會發現平均$|s_{i,j}| = 5$，
平均$m = 10$，因此大部份時候，算法表現和$O(N)$差不多。

\noindent \paragraph{正確性}
如果$\cos(v1,v2)$能超過 \textproc{Threshold}，
表示兩個vector至少有一個字相同(不然$\cos$會為零)，
如果兩個$s_{i,j}$有至少一個字重複，
就至少會屬於某個$\mathit{bucket}$，
所以他們連邊的計算一定不會少，
因此加速之後結果依然一樣。

\subsubsection{Group Ranking}
% ranking 排法
對於每一個$s_{i,j}$，
我們可以利用其原本在$i/|d_i|$來當作其在$d_i$中的rank，
每個group將其擁有的所有$s_{i,j}$的rank做平均當作自己的rank，
這樣就可以對所有搜尋出來的group做排序。

但會發現有些$s_{i,j}$的字只在某本書大量出現，
而且章節也很前面，所以會被排序到前面，
但卻不是重要的內容，
所以我們將group中有幾個「不同」的$d_i$，也考慮進去，
最後group的rank我們定義如下。我們以此penalize出現過少不同$d_i$的group。
\[ \frac
    {\sum \mathit{Rank}(s_{i,j})}
    {|\mathit{group}|\times |\{d_i \,|\, \exists s_{i,j} \in group\}|} \]

\subsubsection{Buzzword 過濾}
% buzzwords 的過濾
Clustering過程中會發現，
很多$s_{i,j}$會因為一些教科書、課程常出現的buzzwords
而被連cluster在一起，
像是Introduction、Chapter、Midterm等等，
我們可以用bucket解決這個問題，
透過直接忽略buzzwords的bucket，
就可以阻止$s_{i,j}$利用buzzword被cluster在一起。
